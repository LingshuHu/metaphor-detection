{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetDependencyParse as GP\n",
    "# Word2vec\n",
    "import gensim\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = WordNetLemmatizer()\n",
    "\n",
    "def text_process(text):\n",
    "    # Remove all the special characters\n",
    "    #document = re.sub(r'\\W', ' ', str(text))\n",
    "    document = re.sub(r'https?:\\S+|http?:\\S', '', str(text))\n",
    "    # remove all single characters\n",
    "    #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    #document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # remove first space\n",
    "    document = re.sub(r'^\\s+', '', document)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # change covid 19 to covid19\n",
    "    document = re.sub(r'covid(\\s+|\\-|\\_)19', 'covid19', document)\n",
    "    # change covid to covid19\n",
    "    document = re.sub(r'covid(\\s+|\\W|$)', 'covid19', document)\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'covid19 and covid19 and covid19 and covid19'"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "text_process(\"covid 19 and covid-19 and covid_19 and covid?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv(\"data/sample4metaphor_jan24-may25_text_filtered.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text2 = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text3 = df.text.apply(lambda x: text_process(x))"
   ]
  },
  {
   "source": [
    "## Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.9 ms, sys: 0 ns, total: 1.9 ms\nWall time: 1.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [_text.split() for _text in df.text2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.word2vec.Word2Vec(size=200, \n",
    "                                            window=5, \n",
    "                                            min_count=2, \n",
    "                                            workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size 2376\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(documents)\n",
    "words = w2v_model.wv.vocab.keys()\n",
    "#print(len(w2v_model.wv[\"u\"]))\n",
    "#print(list(w2v_model.wv.vocab.items())[0:5])\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 770 ms, sys: 24.1 ms, total: 795 ms\nWall time: 513 ms\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(423345, 615456)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('distance', 0.9995676279067993),\n",
       " ('amazing', 0.9995567798614502),\n",
       " ('everything', 0.9995455145835876),\n",
       " ('much', 0.9995318055152893),\n",
       " ('listen', 0.9994614124298096),\n",
       " ('taken', 0.9994546175003052),\n",
       " ('follow', 0.9994485974311829),\n",
       " ('near', 0.9994305372238159),\n",
       " ('nice', 0.9994035959243774),\n",
       " ('scared', 0.9994033575057983)]"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "w2v_model.most_similar(\"covid19\")"
   ]
  },
  {
   "source": [
    "## Noun Pair Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       powerful #perspectives shared here: \"diary of ...\n",
       "1       markets bet fed is pushed to cut rates in coro...\n",
       "2       georgia postponing march 24 presidential prima...\n",
       "3       don't ask me: just caught a glimpse of richard...\n",
       "4       icheoku says if regular flu kills about 60,000...\n",
       "                              ...                        \n",
       "1240    \"we have the best [insert industry/thing] in t...\n",
       "1241    we are still processing building projects/perm...\n",
       "1242    in four u.s. state prisons, nearly 3,300 inmat...\n",
       "1243    i really hate to do this but i just lost one o...\n",
       "1244    @kfile i just hope trump gets coronavirus and ...\n",
       "Name: text, Length: 1245, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "df.text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"data/sample_text.txt\",\"w\") \n",
    "for t in df.text3:\n",
    "    #t + \"\\n\"\n",
    "    file1.writelines(t+\"\\n\")\n",
    "    #file1.write(\"\\n\")\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (PRN\n",
      "      (S\n",
      "        (NP (JJ powerful) (NNS perspectives))\n",
      "        (VP (VBD shared)\n",
      "          (ADVP (RB here))\n",
      "          (NP\n",
      "            (NP (NN diary))\n",
      "            (PP (IN of)\n",
      "              (NP (DT a) (NN pandemic) (NN dee) (NN fearrington))))\n",
      "          (NP (NNP april) (CD 3)))))\n",
      "    (NP\n",
      "      (NP (CD 2020) (JJ https) (NN t))\n",
      "      (NP (AFX co) (NNS bvb65vzzau)))))\n",
      "---\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP (NNS markets))\n",
      "    (VP (VBP bet)\n",
      "      (SBAR\n",
      "        (S\n",
      "          (NP (NN fed))\n",
      "          (VP (VBZ is)\n",
      "            (VP (VBN pushed)\n",
      "              (S\n",
      "                (VP (TO to)\n",
      "                  (VP (VB cut)\n",
      "                    (NP (NNS rates))\n",
      "                    (PP (IN in)\n",
      "                      (NP\n",
      "                        (NP (NN coronavirus) (NN response) (NN reuters) (NN https) (NN t) (NN co) (NN zs4op8knlw))\n",
      "                        (PP (IN via)\n",
      "                          (NP (NNS googlenews)))))))))))))))\n",
      "---\n",
      "(ROOT\n",
      "  (FRAG\n",
      "    (NP (NNP georgia) (NNP postponing))\n",
      "    (NP (NN march) (CD 24))\n",
      "    (NP\n",
      "      (NP (JJ presidential) (JJ primary) (NN election))\n",
      "      (PP (IN amid)\n",
      "        (NP (NN coronavirus) (NNS precautions))))\n",
      "    (NP (NN georgia) (NN public) (NN broadcasting) (NN https) (NN t) (NN co) (NN mh15ipcke0))))\n",
      "---\n",
      "(ROOT\n",
      "  (S\n",
      "    (VP (VB don)\n",
      "      (NP (NN t))\n",
      "      (SBAR (IN ask)\n",
      "        (S\n",
      "          (NP (PRP me))\n",
      "          (ADVP (RB just))\n",
      "          (VP (VBD caught)\n",
      "            (NP\n",
      "              (NP (DT a) (NN glimpse))\n",
      "              (PP (IN of)\n",
      "                (NP (NNP richard) (NN m) (NN nixon))))\n",
      "            (S\n",
      "              (VP (VBG discussing)\n",
      "                (NP (NN something))\n",
      "                (PP (IN on)\n",
      "                  (NP\n",
      "                    (NP (NN abc) (NN tv) (NN while))\n",
      "                    (VP (VBG clicking)\n",
      "                      (PRT (RP over))\n",
      "                      (PP (IN to)\n",
      "                        (NP (DT the) (JJ live) (JJ white) (NN house) (NN coronavirus) (NN briefing)))\n",
      "                      (NP (JJ strange) (NNS times))\n",
      "                      (ADVP (RB indeed)))))))))))))\n",
      "---\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-b754b7740426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print(sen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependency_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/metaphor-detection/GetDependencyParse.py\u001b[0m in \u001b[0;36mdependency_parse\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# needs Java installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mparser_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stanford-parser-full-2020-11-17\"\u001b[0m    \u001b[0;31m# Change if parser is in some other directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mparse_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# List of nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/metaphor-detection/parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(sentence, folder)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mwritefile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/metaphor-detection/parse.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(filepath, folder)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mparser_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Driver function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "nlist = []\n",
    "for sen in df.text3:\n",
    "    #print(sen)\n",
    "    __, nouns = GP.dependency_parse(sen)\n",
    "    nlist.append(nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_output = os.popen(\"stanford-parser-full-2020-11-17\" + '/lexparser.sh \"' + \"this is an apple\" + '\"').read()\n",
    "parser_output = parser_output.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "len(parser_output)\n",
    "print(parser_output[0])"
   ]
  }
 ]
}